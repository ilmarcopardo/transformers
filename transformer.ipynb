{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "## Easy to understand implementation of the Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pe = torch.zeros(self.max_seq_len, self.d_model)\n",
    "\n",
    "        # positional encoder \n",
    "        for j in range(max_seq_len):\n",
    "            for i in range(0, d_model):\n",
    "                if i%2 == 0:\n",
    "                    k = i//2\n",
    "                    wk = 1 / (10000**((2*k)/d_model))\n",
    "                    self.pe[j][i] = torch.sin(torch.tensor(wk*j))\n",
    "                else:\n",
    "                    self.pe[j][i] = torch.cos(torch.tensor(wk*j))\n",
    "\n",
    "        self.pe = self.pe.detach()\n",
    "\n",
    "    def show_pe(self):\n",
    "        print(self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        if x.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceeds max_seq_len\")\n",
    "        \n",
    "        if x.shape[2] != self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        if x.device != self.pe.device:\n",
    "            self.pe = self.pe.to(x.device)  # Move pe to the same device as x\n",
    "\n",
    "        x = x + self.pe[:x.shape[1], :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  2.5116e-02,  9.9968e-01,  6.3096e-04],\n",
      "        [ 9.0930e-01, -4.1615e-01,  5.0217e-02,  9.9874e-01,  1.2619e-03],\n",
      "        [ 1.4112e-01, -9.8999e-01,  7.5285e-02,  9.9716e-01,  1.8929e-03],\n",
      "        [-7.5680e-01, -6.5364e-01,  1.0031e-01,  9.9496e-01,  2.5238e-03],\n",
      "        [-9.5892e-01,  2.8366e-01,  1.2526e-01,  9.9212e-01,  3.1548e-03],\n",
      "        [-2.7942e-01,  9.6017e-01,  1.5014e-01,  9.8866e-01,  3.7857e-03],\n",
      "        [ 6.5699e-01,  7.5390e-01,  1.7493e-01,  9.8458e-01,  4.4167e-03],\n",
      "        [ 9.8936e-01, -1.4550e-01,  1.9960e-01,  9.7988e-01,  5.0476e-03],\n",
      "        [ 4.1212e-01, -9.1113e-01,  2.2415e-01,  9.7455e-01,  5.6786e-03]])\n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoder(max_seq_len=10, d_model=5)\n",
    "pe.show_pe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True positional encoding obtained from the paper\n",
    "Positional Encodings (max_seq_len=10, dim_model=5):<br>\n",
    "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00 0.00000000e+00]<br>\n",
    " [ 8.41470985e-01  5.40302306e-01  2.51162229e-02  9.99684538e-01 6.30957303e-04]<br>\n",
    " [ 9.09297427e-01 -4.16146837e-01  5.02165994e-02  9.98738351e-01 1.26191435e-03]<br>\n",
    " [ 1.41120008e-01 -9.89992497e-01  7.52852930e-02  9.97162035e-01 1.89287090e-03]<br>\n",
    " [-7.56802495e-01 -6.53643621e-01  1.00306487e-01  9.94956586e-01 2.52382670e-03]<br>\n",
    " [-9.58924275e-01  2.83662185e-01  1.25264396e-01  9.92123395e-01 3.15478149e-03]<br>\n",
    " [-2.79415498e-01  9.60170287e-01  1.50143272e-01  9.88664249e-01 3.78573502e-03]<br>\n",
    " [ 6.56986599e-01  7.53902254e-01  1.74927419e-01  9.84581331e-01 4.41668705e-03]<br>\n",
    " [ 9.89358247e-01 -1.45500034e-01  1.99601200e-01  9.79877217e-01 5.04763732e-03]<br>\n",
    " [ 4.12118485e-01 -9.11130262e-01  2.24149048e-01  9.74554875e-01 5.67858558e-03]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(self.d_model, self.d_ff)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.ff2 = torch.nn.Linear(self.d_ff, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        x = self.ff1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layerNorm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layerNorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_k, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.W_q = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "        self.W_k = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "        self.W_v = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "\n",
    "        self.W_O = torch.nn.Linear(in_features = d_k, out_features=d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = False):\n",
    "        if len(Q.shape)==2:\n",
    "            Q.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(K.shape)==2:\n",
    "            K.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(V.shape)==2:\n",
    "            V.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if Q.shape[1] > self.max_seq_len or V.shape[1]> self.max_seq_len or K.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceed max sequence length\")\n",
    "\n",
    "        if Q.shape[2] != self.d_model or K.shape[2] != self.d_model or V.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Tokens dimension do not match model dimension\")\n",
    "\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "\n",
    "        logits = (torch.matmul(Q, torch.transpose(K, 1, 2)) )/ (self.d_k ** 0.5)\n",
    "        if mask is True:\n",
    "            ones = torch.ones(Q.shape[0], Q.shape[1], Q.shape[1]).to(Q.device)\n",
    "            mask = torch.tril(ones)\n",
    "            logits = logits.masked_fill(mask == 0, -float('1e9'))\n",
    "\n",
    "        scores = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        attention = torch.matmul(scores, V)\n",
    "\n",
    "        output = self.W_O(attention)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k=d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([SelfAttentionHead(d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len) for _ in range(num_heads)])\n",
    "\n",
    "        self.W_O = torch.nn.Linear(num_heads*self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = False):\n",
    "        if len(Q.shape)==2:\n",
    "            Q.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(K.shape)==2:\n",
    "            K.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(V.shape)==2:\n",
    "            V.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if Q.shape[1] > self.max_seq_len or V.shape[1]> self.max_seq_len or K.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceed max sequence length\")\n",
    "\n",
    "        if Q.shape[2] != self.d_model or K.shape[2] != self.d_model or V.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Tokens dimension do not match model dimension\")\n",
    "\n",
    "        head_outputs = [head(Q, K, V, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.W_O(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, d_ff, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(num_heads=self.num_heads, d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len)\n",
    "        self.layerNorm = LayerNorm(d_model=self.d_model)\n",
    "        self.ffnn = FeedForwardNN(d_model=self.d_model, d_ff=self.d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "\n",
    "        y = self.multiHeadAttention(x, x, x, False)\n",
    "        \n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        y = self.ffnn(x)\n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, d_ff, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "\n",
    "        self.maskedMultiHeadAttention = MultiHeadAttention(num_heads=self.num_heads, d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len)\n",
    "        self.multiHeadAttention = MultiHeadAttention(num_heads=self.num_heads, d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len)\n",
    "        self.layerNorm = LayerNorm(d_model=self.d_model)\n",
    "        self.ffnn = FeedForwardNN(d_model=self.d_model, d_ff=self.d_ff)\n",
    "\n",
    "    def forward(self, Q, K, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        if Q.shape[2]!=self.d_model or K.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        y = self.maskedMultiHeadAttention(x, x, x, mask=True)\n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        y = self.multiHeadAttention(Q, K, x, mask=False)\n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        y = self.ffnn(x)\n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_encoder_heads, num_decoder_heads, d_model, d_k, d_enc_ff, d_dec_ff, max_seq_len, num_encoder_blocks, num_decoder_blocks, word_embedder = None):\n",
    "        super().__init__()\n",
    "        self.num_encoder_heads = num_encoder_heads\n",
    "        self.num_decoder_heads = num_decoder_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_enc_ff = d_enc_ff\n",
    "        self.d_dec_ff = d_dec_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_encoder_blocks = num_encoder_blocks\n",
    "        self.num_decoder_blocks = num_decoder_blocks\n",
    "        self.word_embedder = word_embedder\n",
    "\n",
    "        self.pe = PositionalEncoder(max_seq_len=self.max_seq_len, d_model=self.d_model)\n",
    "        self.encoders = torch.nn.ModuleList([EncoderBlock(d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len, d_ff=self.d_enc_ff, num_heads=self.num_encoder_heads) for _ in range(self.num_encoder_blocks)])\n",
    "        self.decoders = torch.nn.ModuleList([DecoderBlock(d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len, d_ff=self.d_dec_ff, num_heads=self.num_decoder_heads) for _ in range(self.num_decoder_blocks)])\n",
    "    \n",
    "    def forward(self, x_enc, x_dec):\n",
    "        if len(x_enc.shape) == 2 or len(x_dec.shape)==2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "        \n",
    "        if x_enc.shape[1] > self.max_seq_len or x_dec.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceeds max sequence length\")\n",
    "        \n",
    "        if self.word_embedder is not None:\n",
    "            x_enc = self.word_embedder(x_enc)\n",
    "            x_dec = self.word_embedder(x_dec)\n",
    "\n",
    "        if x_enc.shape[2]!=self.d_model or x_dec.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "\n",
    "        x_enc = self.pe(x_enc)\n",
    "\n",
    "        for block in self.encoders:\n",
    "            x_enc = block(x_enc)\n",
    "\n",
    "        x_dec = self.pe(x_dec)\n",
    "        for block in self.decoders:\n",
    "            x_dec = block(Q=x_enc, K=x_enc, x=x_dec)  # Q, K from encoder, V from decoder\n",
    "        \n",
    "        return x_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From now on, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_ENC_HEADS = 3\n",
    "NUM_DEC_HEADS = 3\n",
    "MAX_SEQ_LEN = 500\n",
    "D_MODEL = 512\n",
    "D_K = 128\n",
    "D_ENC_FF = 256\n",
    "D_DEC_FF = 256\n",
    "NUM_ENC_BLOCKS = 2\n",
    "NUM_DEC_BLOCKS = 2\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = Transformer(\n",
    "    num_encoder_heads = NUM_ENC_HEADS,\n",
    "    num_decoder_heads = NUM_DEC_HEADS,\n",
    "    d_model = D_MODEL,\n",
    "    d_k = D_K,\n",
    "    d_enc_ff = D_ENC_FF,\n",
    "    d_dec_ff = D_DEC_FF,\n",
    "    max_seq_len = MAX_SEQ_LEN,\n",
    "    num_encoder_blocks = NUM_ENC_BLOCKS,\n",
    "    num_decoder_blocks = NUM_DEC_BLOCKS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed!\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "d_model = 512\n",
    "\n",
    "input_tensor = torch.randn(1, seq_len, d_model).to(DEVICE)\n",
    "output = model(x_dec = input_tensor, x_enc = input_tensor)\n",
    "\n",
    "assert output.shape == (1, seq_len, d_model), f\"Unexpected output shape: {output.shape}\"\n",
    "print(\"Shape test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: need more tests!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
