{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "## Easy to understand implementation of the Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pe = torch.zeros(self.max_seq_len, self.d_model)\n",
    "\n",
    "        # positional encoder \n",
    "        for j in range(max_seq_len):\n",
    "            for i in range(0, d_model):\n",
    "                if i%2 == 0:\n",
    "                    k = i//2\n",
    "                    wk = 1 / (10000**((2*k)/d_model))\n",
    "                    self.pe[j][i] = torch.sin(torch.tensor(wk*j))\n",
    "                else:\n",
    "                    self.pe[j][i] = torch.cos(torch.tensor(wk*j))\n",
    "\n",
    "        self.pe = self.pe.detach()\n",
    "\n",
    "    def show_pe(self):\n",
    "        print(self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        if x.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceeds max_seq_len\")\n",
    "        \n",
    "        if x.shape[2] != self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        if x.device != self.pe.device:\n",
    "            self.pe = self.pe.to(x.device)  # Move pe to the same device as x\n",
    "\n",
    "        x = x + self.pe[:x.shape[1], :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoder(max_seq_len=10, d_model=5)\n",
    "pe.show_pe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True positional encoding obtained from the paper\n",
    "Positional Encodings (max_seq_len=10, dim_model=5):<br>\n",
    "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00 0.00000000e+00]<br>\n",
    " [ 8.41470985e-01  5.40302306e-01  2.51162229e-02  9.99684538e-01 6.30957303e-04]<br>\n",
    " [ 9.09297427e-01 -4.16146837e-01  5.02165994e-02  9.98738351e-01 1.26191435e-03]<br>\n",
    " [ 1.41120008e-01 -9.89992497e-01  7.52852930e-02  9.97162035e-01 1.89287090e-03]<br>\n",
    " [-7.56802495e-01 -6.53643621e-01  1.00306487e-01  9.94956586e-01 2.52382670e-03]<br>\n",
    " [-9.58924275e-01  2.83662185e-01  1.25264396e-01  9.92123395e-01 3.15478149e-03]<br>\n",
    " [-2.79415498e-01  9.60170287e-01  1.50143272e-01  9.88664249e-01 3.78573502e-03]<br>\n",
    " [ 6.56986599e-01  7.53902254e-01  1.74927419e-01  9.84581331e-01 4.41668705e-03]<br>\n",
    " [ 9.89358247e-01 -1.45500034e-01  1.99601200e-01  9.79877217e-01 5.04763732e-03]<br>\n",
    " [ 4.12118485e-01 -9.11130262e-01  2.24149048e-01  9.74554875e-01 5.67858558e-03]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(self.d_model, self.d_ff)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.ff2 = torch.nn.Linear(self.d_ff, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "        \n",
    "        x = self.ff1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layerNorm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layerNorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_k, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.W_q = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "        self.W_k = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "        self.W_v = torch.nn.Linear(in_features=d_model, out_features=d_k)\n",
    "\n",
    "        self.W_O = torch.nn.Linear(in_features = d_k, out_features=d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = False):\n",
    "        if len(Q.shape)==2:\n",
    "            Q.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(K.shape)==2:\n",
    "            K.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(V.shape)==2:\n",
    "            V.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if Q.shape[1] > self.max_seq_len or V.shape[1]> self.max_seq_len or K.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceed max sequence length\")\n",
    "\n",
    "        if Q.shape[2] != self.d_model or K.shape[2] != self.d_model or V.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Tokens dimension do not match model dimension\")\n",
    "\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "\n",
    "        logits = (torch.matmul(Q, torch.transpose(K, 1, 2)) )/ (self.d_k ** 0.5)\n",
    "        if mask is True:\n",
    "            ones = torch.ones(Q.shape[0], Q.shape[1], Q.shape[1]).to(Q.device)\n",
    "            mask = torch.tril(ones)\n",
    "            logits = logits.masked_fill(mask == 0, -float('1e9'))\n",
    "\n",
    "        scores = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        attention = torch.matmul(scores, V)\n",
    "\n",
    "        output = self.W_O(attention)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k=d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([SelfAttentionHead(d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len) for _ in range(num_heads)])\n",
    "\n",
    "        self.W_O = torch.nn.Linear(num_heads*self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = False):\n",
    "        if len(Q.shape)==2:\n",
    "            Q.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(K.shape)==2:\n",
    "            K.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if len(V.shape)==2:\n",
    "            V.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if Q.shape[1] > self.max_seq_len or V.shape[1]> self.max_seq_len or K.shape[1]>self.max_seq_len:\n",
    "            raise Exception(\"Number of tokens exceed max sequence length\")\n",
    "\n",
    "        if Q.shape[2] != self.d_model or K.shape[2] != self.d_model or V.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Tokens dimension do not match model dimension\")\n",
    "\n",
    "        head_outputs = [head(Q, K, V, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.W_O(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, d_ff, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(num_heads=self.num_heads, d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len)\n",
    "        self.layerNorm = LayerNorm(d_model=self.d_model)\n",
    "        self.ffnn = FeedForwardNN(d_model=self.d_model, d_ff=self.d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "\n",
    "        y = self.multiHeadAttention(x, x, x, False)\n",
    "        \n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        y = self.ffnn(x)\n",
    "        x = x + y\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, d_ff, max_seq_len, num_blocks, word_embedder = None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_blocks = num_blocks\n",
    "        self.word_embedder = word_embedder\n",
    "\n",
    "        self.pe = PositionalEncoder(max_seq_len=self.max_seq_len, d_model=self.d_model)\n",
    "        self.encoders = torch.nn.ModuleList([EncoderBlock(d_model=self.d_model, d_k=self.d_k, max_seq_len=self.max_seq_len, d_ff=self.d_ff, num_heads=self.num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(\"Warning: batch size not present\")\n",
    "        \n",
    "        if x.shape[1] > self.max_seq_len:\n",
    "            print(x.shape)\n",
    "            raise Exception(\"Number of tokens exceeds max sequence length\")\n",
    "        \n",
    "        if self.word_embedder is not None:\n",
    "            x = self.word_embedder(x)\n",
    "\n",
    "        if x.shape[2]!=self.d_model:\n",
    "            raise Exception(\"Token dimension do not match model dimension\")\n",
    "\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for block in self.encoders:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here on, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_HEADS = 3\n",
    "MAX_SEQ_LEN = 500\n",
    "D_MODEL = 512\n",
    "D_K = 128\n",
    "D_FF = 256\n",
    "NUM_BLOCKS = 2\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed!\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "d_model = 512\n",
    "\n",
    "model = EncoderWrapper(num_heads=NUM_HEADS, num_blocks=NUM_BLOCKS, d_model=D_MODEL, d_k=D_K, d_ff=D_FF, max_seq_len=MAX_SEQ_LEN).to(DEVICE)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "\n",
    "input_tensor = torch.randn(1, seq_len, d_model).to(DEVICE)\n",
    "output = model(input_tensor)\n",
    "\n",
    "assert output.shape == (1, seq_len, d_model), f\"Unexpected output shape: {output.shape}\"\n",
    "print(\"Shape test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.31085100769996643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.012060209177434444\n",
      "Epoch 20: Loss = 0.00608134875074029\n",
      "Epoch 30: Loss = 0.0060678087174892426\n",
      "Epoch 40: Loss = 0.005880665499716997\n",
      "Epoch 50: Loss = 0.003627615747973323\n",
      "Epoch 60: Loss = 0.002997001400217414\n",
      "Epoch 70: Loss = 0.002640163293108344\n",
      "Epoch 80: Loss = 0.0024372104089707136\n",
      "Epoch 90: Loss = 0.0022794497199356556\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, seq_len, d_model).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(0,100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = loss_fn(output, input_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1905, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(torch.abs(input_tensor-model(input_tensor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: need more tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
